{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scipy==1.6.3\n",
    "pip install stable-baselines3[extra]\n",
    "apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
    "pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "\n",
    "pip install -U kora\n",
    "pip install tensorboard\n",
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn as nn\n",
    "from stable_baselines3 import PPO, A2C, SAC, DQN, DDPG\n",
    "import gym\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython import display #as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "import tensorflow as tf\n",
    "from google.colab import files\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "from stable_baselines3.common.type_aliases import GymStepReturn\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import torch as th\n",
    "import seaborn as sb\n",
    "import itertools\n",
    "from itertools import product\n",
    "from matplotlib import colors\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.gridspec as GridSpec\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy import signal\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = [\"Monitor\", \"get_monitor_files\", \"load_results\"]\n",
    "\n",
    "def create_model(env: gym.Env, algo: str, hyperparams:dict):\n",
    "        return {\"ppo\": PPO,\n",
    "                \"a2c\": A2C,\n",
    "                \"dqn\": DQN,\n",
    "                \"sac\": SAC,\n",
    "                \"ddpg\": DDPG}[algo](policy='MlpPolicy', env=env, **hyperparams)\n",
    "\n",
    "#%%\n",
    "\n",
    "def load_model(algo: str, net_file:str):\n",
    "    return {\"ppo\": PPO, \"a2c\":A2C, \"dqn\":DQN, \"sac\":SAC, \"ddpg\":DDPG}[algo].load(net_file)\n",
    "\n",
    "#%%\n",
    "\n",
    "def load_hyperparams(env_id:str, algo:str, json_file:str):\n",
    "    return get_hyperparams(env_id=env_id, algo=algo, hyperparams=load_dict(filename=json_file))\n",
    "\n",
    "#%%\n",
    "\n",
    "\n",
    "def get_hyperparams(env_id:str, algo:str, hyperparams:dict)->dict:\n",
    "    #policy_kwargs=dict\n",
    "    policy_kwargs = get_extractor_hyperparams(env_id=env_id, hyperparams=hyperparams)\n",
    "    is_actor_critic=(algo=='ppo' or algo=='a2c')\n",
    "    if 'n_neurons' in hyperparams.keys() and 'n_layers' in hyperparams.keys():\n",
    "        if is_actor_critic:\n",
    "            net_arch = [dict(pi=list([int(hyperparams['n_neurons'])] * int(hyperparams['n_layers'])),\n",
    "                             vf=list([int(hyperparams['n_neurons'])] * int(hyperparams['n_layers'])))]\n",
    "        else:\n",
    "            net_arch = list([int(hyperparams['n_neurons'])] * int(hyperparams['n_layers']))\n",
    "        policy_kwargs['net_arch']=net_arch\n",
    "        del hyperparams['n_neurons'], hyperparams['n_layers']\n",
    "    if 'activation_fn' in hyperparams.keys():\n",
    "        policy_kwargs['activation_fn'] = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU, \"elu\": nn.ELU, \"leaky_relu\": nn.LeakyReLU}[hyperparams['activation_fn']]\n",
    "        del hyperparams['activation_fn']\n",
    "    if is_actor_critic:\n",
    "        policy_kwargs['ortho_init'] = False\n",
    "    kwargs = dict(policy_kwargs=policy_kwargs)\n",
    "\n",
    "    if 'action_noise' in hyperparams.keys():\n",
    "        n_actions=0\n",
    "        env = gym.make(env_id)\n",
    "        if env.action_space.shape != ():\n",
    "            n_actions = env.action_space.shape[-1]\n",
    "        env.close()\n",
    "        del env\n",
    "        if hyperparams['action_noise'] == \"normal\":\n",
    "            hyperparams[\"action_noise\"] = NormalActionNoise(mean=np.zeros(n_actions), sigma=hyperparams['noise_std'] * np.ones(n_actions))\n",
    "        elif hyperparams['action_noise'] == \"ornstein-uhlenbeck\":\n",
    "            hyperparams[\"action_noise\"] = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=hyperparams['noise_std'] * np.ones(n_actions))\n",
    "        del hyperparams['action_noise'], hyperparams[\"noise_std\"]\n",
    "    for h in hyperparams.keys():\n",
    "        kwargs[h] = hyperparams[h]\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "\n",
    "def load_dict(filename:str)->dict:\n",
    "    data={}\n",
    "    with open(filename) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "#%%\n",
    "\n",
    "def get_extractor_hyperparams(env_id:str, hyperparams:dict)->dict:\n",
    "    policy_kwargs={}\n",
    "    if 'order' in hyperparams.keys() or 'n_tilings' in hyperparams.keys() or 'normalize' in hyperparams.keys() or 'octa' in hyperparams.keys() or 'fourier_nn' in hyperparams.keys():\n",
    "        features_extractor_kwargs = dict(env_id=env_id, device=get_device())\n",
    "        if 'normalize' in hyperparams.keys():\n",
    "            del hyperparams['normalize']\n",
    "        if 'unbounded_normalizer' in hyperparams.keys():\n",
    "            features_extractor_kwargs['unbounded_normalizer']={\"tanh\": Unbounded_Normalizer.tanh, \"arctan\": Unbounded_Normalizer.arctan}[hyperparams['unbounded_normalizer']]\n",
    "            del hyperparams['unbounded_normalizer']\n",
    "        if 'unbounded_normalizer_scale' in hyperparams.keys():\n",
    "            features_extractor_kwargs['unbounded_normalizer_scale'] = hyperparams['unbounded_normalizer_scale']\n",
    "            del hyperparams['unbounded_normalizer_scale']\n",
    "        if 'order' in hyperparams.keys():\n",
    "            features_extractor_kwargs['order']=hyperparams['order']\n",
    "            del hyperparams['order']\n",
    "            for env in UNBOUNDED_NORMALIZER_DEFAULT.keys():\n",
    "                if env_id.rfind(env) != -1:\n",
    "                    features_extractor_kwargs.update(dict(unbounded_normalizer=Unbounded_Normalizer.arctan,\n",
    "                                                          unbounded_normalizer_scale=UNBOUNDED_NORMALIZER_DEFAULT[env]))\n",
    "            if 'gaussian_std' in hyperparams.keys():\n",
    "                features_extractor_kwargs['gaussian_std'] = hyperparams['gaussian_std']\n",
    "                del hyperparams['gaussian_std']\n",
    "                policy_kwargs = dict(features_extractor_class=Random_Fourier_Feature_Extractor, features_extractor_kwargs=features_extractor_kwargs)\n",
    "            elif 'poly' in hyperparams.keys():\n",
    "                del hyperparams['poly']\n",
    "                policy_kwargs = dict(features_extractor_class=Poly_Basis_Extractor, features_extractor_kwargs=features_extractor_kwargs)\n",
    "            elif 'chebyshev' in hyperparams.keys():\n",
    "                del hyperparams['chebyshev']\n",
    "                policy_kwargs = dict(features_extractor_class=Chebyshev_Basis_Extractor, features_extractor_kwargs=features_extractor_kwargs)\n",
    "            else:\n",
    "                policy_kwargs = dict(features_extractor_class=Fourier_Basis_Extractor, features_extractor_kwargs=features_extractor_kwargs)\n",
    "\n",
    "        elif 'n_tilings' in hyperparams.keys():\n",
    "            features_extractor_kwargs['n_tilings'] = hyperparams['n_tilings']\n",
    "            features_extractor_kwargs['n_tiles_dim'] = hyperparams['n_tiles_dim']\n",
    "            del hyperparams['n_tilings'], hyperparams['n_tiles_dim']\n",
    "            policy_kwargs = dict(features_extractor_class=Tile_Coding_Extractor, features_extractor_kwargs=features_extractor_kwargs)\n",
    "        elif 'octa' in hyperparams.keys():\n",
    "            del hyperparams['octa']\n",
    "            policy_kwargs = dict(features_extractor_class=Fourier_Octagonal_Extractor, features_extractor_kwargs=features_extractor_kwargs)\n",
    "\n",
    "        elif 'fourier_nn' in hyperparams.keys():\n",
    "            del hyperparams['fourier_nn']\n",
    "            policy_kwargs = dict(features_extractor_class=Fourier_NN_Extractor, features_extractor_kwargs=features_extractor_kwargs)\n",
    "        else:\n",
    "            policy_kwargs = dict(features_extractor_class=Normalizer_Extractor, features_extractor_kwargs=features_extractor_kwargs)\n",
    "    return policy_kwargs\n",
    "\n",
    "#################### POUR INTRODUIRE DES BRUITS GAUSSIENS PENDANT L'ENTRAINEMENT###########################################\n",
    "GAUSSIAN_NOISE_STD=dict(MountainCar=np.array([0.018,0.0014])) #CartPole=np.array([0.0025, 0.01, 0.013, 0.01]),\n",
    "\n",
    "class Gaussian_Noise:\n",
    "    def __init__(self, env_id:str, device:th.device=th.device(\"cuda:0\")):\n",
    "        env = gym.make(env_id)\n",
    "        self._n_dim = len(env.observation_space.low)\n",
    "        env.close()\n",
    "        del env\n",
    "        self._device = device\n",
    "        self._mean, self._std = np.zeros((self._n_dim)), np.ones((self._n_dim))\n",
    "        for env in GAUSSIAN_NOISE_STD.keys():\n",
    "            if env_id.rfind(env) != -1:\n",
    "                self._std = GAUSSIAN_NOISE_STD[env]\n",
    "\n",
    "        self._mean, self._std = th.tensor(self._mean,device=self._device,dtype=th.float32),th.tensor(self._std,device=self._device,dtype=th.float32)\n",
    "\n",
    "    def get_feature(self, obs) -> th.Tensor:  #ajout d'un bruit tiré d'une loi normale de moyenne 0 et std le vecteur gaussian_noise_std\n",
    "        return th.normal(mean=self._mean, std=self._std)+obs\n",
    "\n",
    "    @property\n",
    "    def n_dim(self):\n",
    "        return self._n_dim\n",
    "\n",
    "\n",
    "class Gaussian_Noise_Extractor(BaseFeaturesExtractor):\n",
    "\n",
    "    def __init__(self, observation_space:gym.Space, env_id:str, device:th.device=th.device(\"cuda:0\")):\n",
    "        self._gaussian_noise = Gaussian_Noise(env_id=env_id, device=device)\n",
    "        super().__init__(observation_space, self._gaussian_noise.n_dim)\n",
    "        self._flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, obs: th.Tensor) -> th.Tensor:\n",
    "        return self._flatten(self._gaussian_noise.get_feature(obs))\n",
    "\n",
    "\n",
    "############## POUR LE SAUVEGARDE DES réseaux de neurone #########################\n",
    "class Save_Net_Callback(BaseCallback):\n",
    "\n",
    "    def __init__(self, log_dir:str, check_freq:int, verbose=0):\n",
    "        super(Save_Net_Callback, self).__init__(verbose)\n",
    "        self._check_freq = check_freq\n",
    "        self._log_dir=log_dir\n",
    "        #create_directory(path=self._log_dir)\n",
    "\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self._check_freq == 0:\n",
    "            self.model.save(self._log_dir+str(self.n_calls))\n",
    "        return True\n",
    "\n",
    "class Save_Net_Callback_By_Episode(BaseCallback):\n",
    "\n",
    "    def __init__(self, log_dir:str, verbose=0):\n",
    "        super(Save_Net_Callback_By_Episode, self).__init__(verbose)\n",
    "        self._log_dir=log_dir\n",
    "        self.count=1\n",
    "        #create_directory(path=self._log_dir)\n",
    "\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        done_array = np.array(self.locals.get(\"done\") if self.locals.get(\"done\") is not None else self.locals.get(\"dones\"))\n",
    "        fin_episode = np.sum(done_array).item() #fin_episode=1 si l'episode est fini sinon on aura 0. Ce vecteur reprend de vide \n",
    "                                                #à chaque début d'episode\n",
    "\n",
    "        #if(fin_episode>=self.count): #pour éviter == et problème de précision #https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/common/callbacks.html#BaseCallback\n",
    "        if fin_episode>=1:\n",
    "          self.model.save(self._log_dir+str(self.count))\n",
    "          self.count+=1\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction qui fait les entrainements (selon des hyperparamètres entrés) et sauvegarde les NN dans un fichier .zip\n",
    "#on sauvegarde le dernier modèle de chaque entrainement\n",
    "#env_id: nom du jeu\n",
    "#algo:algorithme (dqn,ppo,a2c..)\n",
    "#json_file:nom du fichier (input) des hyperparamètres\n",
    "#nb_of_trainings: nombre d'entrainements à effectuer\n",
    "#n_timesteps: nombre de pas de temps\n",
    "#noise: variable booléenne (true si on introduit un bruit gaussien aux entrainements, false sinon)\n",
    "\n",
    "def Train_And_SaveNN_hyp(env_id:str, algo:str, json_file:str, nb_of_trainings:int, n_timesteps:int, noise:bool):\n",
    "\n",
    "    list_of_trainings=[] #liste vide où on va sauvegarder chacun des entrainements \n",
    "    hyperparam=load_hyperparams(env_id, algo, json_file)\n",
    "    if noise:\n",
    "        policy_kwargs = dict(\n",
    "        features_extractor_class=Gaussian_Noise_Extractor,\n",
    "        features_extractor_kwargs=dict(env_id=env_id),\n",
    "        )\n",
    "        hyperparam['policy_kwargs']={**hyperparam['policy_kwargs'], **policy_kwargs }\n",
    "    for i in range(1,nb_of_trainings+1):\n",
    "      log_dir=\"log\"+str(i)+\"/\" #pour avoir un log de chq entrainement\n",
    "      os.makedirs(log_dir, exist_ok=True)\n",
    "      env=Monitor(gym.make(env_id), log_dir, allow_early_resets=True)\n",
    "\n",
    "\n",
    "      model=create_model(env,algo,hyperparam)\n",
    "      callback = Save_Net_Callback_By_Episode(log_dir=log_dir,  verbose=0)\n",
    "\n",
    "      model.learn(total_timesteps=n_timesteps, callback=callback)\n",
    "\n",
    "      #récupération des rewards:\n",
    "      training_rewards = env.get_episode_rewards()[1:] #supression de la première episode #rewards à la première episode dépendent de l'initialisation(semble être parfait)\n",
    "      #concatenation des rewards\n",
    "      list_of_trainings.append(training_rewards)\n",
    "\n",
    "    # transformer en matrice (même nombre de colonnes)\n",
    "    #trouver taille min des listes dans newlist\n",
    "    t_min=min([len(x) for x in list_of_trainings])\n",
    "    #garder uniquement t valeurs pour chaque entrainement\n",
    "    matrice_rewards=[x[:t_min] for x in list_of_trainings]\n",
    "\n",
    "    #on obtient une matrice des rewards qui comporte en ligne: chaque entrainement, en colonne: les timesteps\n",
    "\n",
    "    #sauvegarder dans un fichier les récompenses des entrainements\n",
    "    np.savetxt(env_id+\"_\"+algo+\".txt\",matrice_rewards)\n",
    "    files.download(env_id+\"_\"+algo+\".txt\")\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour pouvoir appliquer les fonctions qui génèrent les rollouts, il faut y avoir les logs de l'entraînement\n",
    "def get_rollouts(path:str, session_id:int, env_id:str, algo:str, net:str, n_rollouts:int, init_states_file:str, verbose:bool=False)->None:\n",
    "    init_states=np.loadtxt(init_states_file)\n",
    "    env = gym.make(env_id)\n",
    "    model = load_model(algo, net)\n",
    "    rollout_i = []\n",
    "    for i in range(n_rollouts):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        env._elapsed_steps=0\n",
    "        obs=init_states[i]\n",
    "        env.unwrapped.state=obs\n",
    "        cumul_reward=0.0\n",
    "        \n",
    "        while not (done):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            cumul_reward+=reward\n",
    "            \n",
    "        rollout_i.append(cumul_reward)\n",
    "        if verbose:\n",
    "            print(\"\\rRun {}/{}\".format(i + 1, n_rollouts), end=\"\")\n",
    "    if verbose:\n",
    "        print(\"\")\n",
    "    \n",
    "    #sauvegarder le vecteur contenant les récompenses\n",
    "    np.savetxt(path,rollout_i)\n",
    "    env.close()\n",
    "    del model, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduction du bruit gaussien aux rollouts: var_position=1.8 et var_velocity=0.14\n",
    "def get_noisy_rollouts(path:str, session_id:int, env_id:str, algo:str, net:str, n_rollouts:int, init_states_file:str, var_position:int,  var_velocity:int, var_percent:int, verbose:bool=False)->None:\n",
    "    init_states=np.loadtxt(init_states_file)\n",
    "    env = gym.make(env_id)\n",
    "    _n_dim = len(env.observation_space.low)\n",
    "    model = load_model(algo, net)\n",
    "    rollout_i = []\n",
    "    for i in range(n_rollouts):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        env._elapsed_steps=0\n",
    "        obs=init_states[i]\n",
    "        mu_noise, sigma_noise = np.zeros(_n_dim), np.ones(_n_dim)\n",
    "        sigma_noise= sigma_noise*[var_position*var_percent,var_velocity*var_percent]\n",
    "        obs=obs+np.random.normal(mu_noise, sigma_noise)\n",
    "        env.unwrapped.state=obs\n",
    "        cumul_reward=0.0\n",
    "\n",
    "        while not (done):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            obs=obs+np.random.normal(mu_noise, sigma_noise)\n",
    "            cumul_reward+=reward\n",
    "            \n",
    "        rollout_i.append(cumul_reward)\n",
    "        if verbose:\n",
    "            print(\"\\rRun {}/{}\".format(i + 1, n_rollouts), end=\"\")\n",
    "    if verbose:\n",
    "        print(\"\")\n",
    "    np.savetxt(path,rollout_i)\n",
    "    env.close()\n",
    "    del model, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollouts_on_fixed_states(env_id:str, session_id:int, algo:str,  n_rollouts:int,  nb_of_trainings:int, init_states:str):\n",
    "  \n",
    "  path_init=algo+\"_hyp_rollout.txt\"\n",
    "  #avant on faisait le directory (rollouts) puis on regarde le log\n",
    "  #ici on a plusieurs log dont chacun doit avoir un directory rollouts\n",
    "  os.makedirs(\"rollouts\", exist_ok=True) #à chaque log et à chaque fichier du log correspond un rollout\n",
    "  for i in range(1,nb_of_trainings+1):\n",
    "      files_list=os.listdir(\"log\"+str(i)) #pour chacun des log sauvegardé\n",
    "      \n",
    "      for n in files_list:\n",
    "        if n.endswith(\".zip\"):\n",
    "            path=\"rollouts/ent\"+str(i)+\"_ep\"+os.path.splitext(n)[0]+\"_\"+path_init\n",
    "            get_rollouts(path=path, session_id=session_id, env_id=env_id, algo=algo, net=\"log\"+str(i)+\"/\"+os.path.splitext(n)[0], n_rollouts=n_rollouts, init_states_file=init_states)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dans cette fonction, le but est de faire les rollouts sur la politique optimale de chaque entrainement\n",
    "def rollouts_on_fixed_states_optimalPol( env_id:str, session_id:int, algo:str,  n_rollouts:int,  nb_of_trainings:int, init_states:str):\n",
    "  \n",
    "  path_init=algo+\"_hyp_opt_rollout.txt\"\n",
    "  #avant on faisait le directory (rollouts) puis on regarde le log\n",
    "  #ici on a plusieurs log dont chacun doit avoir un directory rollouts\n",
    "  os.makedirs(\"rollouts/\", exist_ok=True) #à chaque log et à chaque fichier du log correspond un rollout\n",
    "  for i in range(1,nb_of_trainings+1):\n",
    "      files_list=os.listdir(\"log\"+str(i)) #pour chacun des log sauvegardé\n",
    "      file=\"\"\n",
    "      max_ep=0 #max_ep pour détecter le dernier episode correspondant à la politique \n",
    "      for n in files_list:\n",
    "        if n.endswith(\".zip\"):\n",
    "          episode=os.path.splitext(n)[0]\n",
    "          episode=int(episode)\n",
    "          if episode>max_ep:\n",
    "            max_ep=episode\n",
    "      path=\"rollouts/ent\"+str(i)+\"_ep\"+str(max_ep)+\"_\"+path_init\n",
    "      get_rollouts(path=path, session_id=session_id, env_id=env_id, algo=algo, net=\"log\"+str(i)+\"/\"+str(max_ep), n_rollouts=n_rollouts, init_states_file=init_states)\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollouts_on_fixed_states_optimalPol( env_id:str, session_id:int, algo:str,  n_rollouts:int,  nb_of_trainings:int, init_states:str, var_position:int,  var_velocity:int, var_percent:int):\n",
    "  \n",
    "  path_init=algo+\"_hyp_noisy_rollout.txt\"\n",
    "  #avant on faisait le directory (rollouts) puis on regarde le log\n",
    "  #ici on a plusieurs log dont chacun doit avoir un directory rollouts\n",
    "  os.makedirs(\"noisy_rollouts/\", exist_ok=True) #à chaque log et à chaque fichier du log correspond un rollout\n",
    "  for i in range(1,nb_of_trainings+1):\n",
    "      files_list=os.listdir(\"log\"+str(i)) #pour chacun des log sauvegardé\n",
    "      file=\"\"\n",
    "      max_ep=0\n",
    "      for n in files_list:\n",
    "        if n.endswith(\".zip\"):\n",
    "          episode=os.path.splitext(n)[0]\n",
    "          episode=int(episode)\n",
    "          if episode>max_ep:\n",
    "            max_ep=episode\n",
    "      path=\"noisy_rollouts/ent\"+str(i)+\"_ep\"+str(max_ep)+\"_\"+path_init\n",
    "      get_noisy_rollouts(path=path, session_id=session_id, env_id=env_id, algo=algo, net=\"log\"+str(i)+\"/\"+str(max_ep), n_rollouts=n_rollouts, init_states_file=init_states, var_position=var_position,  var_velocity=var_velocity, var_percent=var_percent)\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour les rollouts donnant les détails de la position et de la vitesse (Environnement MountainCar)\n",
    "\n",
    "def get_rollouts_details(path:str, session_id:int, env_id:str, algo:str, net:str, n_rollouts:int, init_states_file:str, verbose:bool=False)->None:\n",
    "    init_states=np.loadtxt(init_states_file)\n",
    "    env = gym.make(env_id) \n",
    "    model = load_model(algo, net)\n",
    "    obs_i = []\n",
    "    reward_i = []\n",
    "    list_count=[]\n",
    "    rollout_listNum=[]\n",
    "    for i in range(n_rollouts):\n",
    "        done = False\n",
    "        env.reset()\n",
    "        env._elapsed_steps=0\n",
    "        obs=init_states[i]\n",
    "        env.unwrapped.state=obs\n",
    "        # on veut avoir dans un vecteur les observations qu'on reçoit au cours du temps ainsi que les rewards dans un autre \n",
    "        #vecteur (vecteur reward au cours du temps)\n",
    "        while not (done):\n",
    "            obs_i.append(obs)\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)         \n",
    "         \n",
    "            reward_i.append(reward)\n",
    "            rollout_listNum.append(i)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\rRun {}/{}\".format(i + 1, n_rollouts), end=\"\")\n",
    "    if verbose:\n",
    "        print(\"\")\n",
    "\n",
    "    with open(path,\"w\") as f:\n",
    "      k=0\n",
    "      for (obs,rew,rollout) in zip(obs_i,reward_i,rollout_listNum):\n",
    "        if k>0:\n",
    "          f.write(\"\\n\")\n",
    "        k=k+1  \n",
    "        f.write(\"{0},{1},{2}\".format(obs,rew,rollout))\n",
    "    \n",
    "    env.close()\n",
    "    del model, env\n",
    "    \n",
    "def rollouts_details(env_id:str, session_id:int, algo:str,  n_rollouts:int,  nb_of_trainings:int, init_states:str):\n",
    "  \n",
    "  path_init=algo+\"_hyp_rollout_details.txt\"\n",
    "  #avant on faisait le directory (rollouts) puis on regarde le log\n",
    "  #ici on a plusieurs log dont chacun doit avoir un directory rollouts\n",
    "  os.makedirs(\"rollouts\", exist_ok=True) #à chaque log et à chaque fichier du log correspond un rollout\n",
    "  for i in range(1,nb_of_trainings+1):\n",
    "      files_list=os.listdir(\"log\"+str(i)) #pour chacun des log sauvegardé\n",
    "      file=\"\"\n",
    "      max_ep=0\n",
    "      for n in files_list:\n",
    "        if n.endswith(\".zip\"):\n",
    "          episode=os.path.splitext(n)[0]\n",
    "          episode=int(episode)\n",
    "          if episode>max_ep:\n",
    "            max_ep=episode\n",
    "      path=\"rollouts/ent\"+str(i)+\"_ep\"+str(max_ep)+\"_\"+path_init\n",
    "      get_rollouts_details(path=path, session_id=session_id, env_id=env_id, algo=algo, net=\"log\"+str(i)+\"/\"+str(max_ep), n_rollouts=n_rollouts, init_states_file=init_states)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
